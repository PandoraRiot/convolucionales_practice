{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNsH6EW5ocTOVFlSBstq5Ww"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## üéØ 1) Funci√≥n de P√©rdida (Loss), Optimizadores y Scheduler"],"metadata":{"id":"5D4Q7ytXXDVS"}},{"cell_type":"markdown","source":["La funci√≥n de p√©rdida mide **qu√© tan mal est√° prediciendo tu modelo**. Es una m√©trica usada por el optimizador para ajustar los pesos.  \n","\n","**Matem√°ticamente, buscamos minimizarla durante el entrenamiento.**"],"metadata":{"id":"Df3v5CsVlDev"}},{"cell_type":"markdown","source":["## 2) üìê Fundamento Matem√°tico"],"metadata":{"id":"X5IcrQRgoqF2"}},{"cell_type":"markdown","source":["\n","### 2.1) C√°lculo de la funci√≥n `nn.CrossEntropyLoss` paso a paso\n","\n","Dado un vector de *logits* (valores sin normalizar):\n","\n","```python\n","logits = torch.tensor([[1.0, 2.0, 0.1]])\n","labels = torch.tensor([1])  # Clase real\n","```\n","\n","La funci√≥n `nn.CrossEntropyLoss` aplica internamente la funci√≥n **Softmax** para convertir los logits en probabilidades:\n","\n","$$\n","\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n","$$\n","\n","Aplicando Softmax sobre los logits obtenemos:\n","\n","$$\n","\\text{Probabilidades} = [0.2424,\\ 0.6590,\\ 0.0986]\n","$$\n","\n","La clase correcta (seg√∫n la etiqueta) es la **clase 1**, con una probabilidad asignada de:\n","\n","$$\n","p_{\\text{correcta}} = 0.6590\n","$$\n","\n","Entonces, la **funci√≥n de p√©rdida Cross Entropy** se define como:\n","\n","$$\n","\\mathcal{L}(y, \\hat{y}) = -\\log(\\hat{y}_{\\text{clase correcta}})\n","$$\n","\n","Aplicando los valores:\n","\n","$$\n","\\mathcal{L}(1, [0.2424, 0.6590, 0.0986]) = -\\log(0.6590) \\approx 0.4170\n","$$\n","\n","üëâ Esta p√©rdida ser√° **menor cuanto m√°s alta sea la probabilidad asignada a la clase correcta**, y **mayor si el modelo se equivoca o no est√° seguro**."],"metadata":{"id":"dC7SiRkHo5wn"}},{"cell_type":"markdown","source":["### 2.1) Implementaci√≥n de C√≥digo"],"metadata":{"id":"XeZ-9N8hpbuE"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"sc8TPdLjpen1","executionInfo":{"status":"ok","timestamp":1745504918124,"user_tz":180,"elapsed":5562,"user":{"displayName":"Carlos Bustillo","userId":"14895763228834044971"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Simulamos una predicci√≥n: logits para 3 clases\n","logits = torch.tensor([[1.0, 2.0, 0.1]])  # sin aplicar softmax\n","labels = torch.tensor([1])  # clase real\n","\n","criterion = nn.CrossEntropyLoss()\n","loss = criterion(logits, labels)\n","\n","print(f\"CrossEntropyLoss: {loss.item():.4f}\")\n","print(\"Probabilidades:\", F.softmax(logits, dim=1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MePoj3dmp3Ju","executionInfo":{"status":"ok","timestamp":1745504939878,"user_tz":180,"elapsed":143,"user":{"displayName":"Carlos Bustillo","userId":"14895763228834044971"}},"outputId":"1c831cfc-0ea0-4037-8a9d-fc0a248ce4d2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["CrossEntropyLoss: 0.4170\n","Probabilidades: tensor([[0.2424, 0.6590, 0.0986]])\n"]}]},{"cell_type":"markdown","source":["## 3) üß™ Ejemplos"],"metadata":{"id":"uWzT9jjjos4q"}},{"cell_type":"markdown","source":["### 3.1) ¬øQu√© es un Optimizador?"],"metadata":{"id":"5Nz0QnfJqe_m"}},{"cell_type":"markdown","source":["Un optimizador se encarga de **actualizar los pesos de la red** para minimizar la funci√≥n de p√©rdida.  \n","Ejemplos populares: `SGD`, `Adam`, `RMSprop`.  \n","Utilizan el gradiente para decidir **c√≥mo mover los pesos en cada paso** del entrenamiento."],"metadata":{"id":"nIXbT9v8qcVS"}},{"cell_type":"markdown","source":["### 3.2) ¬øQu√© es un Scheduler?"],"metadata":{"id":"YHPVk6A1rLbn"}},{"cell_type":"markdown","source":["Un scheduler (programador de tasa de aprendizaje) ajusta din√°micamente el **learning rate**.  \n","Esto puede ayudar a converger mejor y m√°s r√°pido, o evitar estancarse.  \n","\n","Ejemplo: reducir el `lr` cuando el modelo deja de mejorar."],"metadata":{"id":"o78BusyPrPzO"}},{"cell_type":"markdown","source":["### 3.3) Implementaci√≥n de C√≥digo de Optimizer + Scheduler"],"metadata":{"id":"XYV4f58KqhXs"}},{"cell_type":"code","source":["import torch.optim as optim\n","from torch.optim import lr_scheduler"],"metadata":{"id":"N5XJJgJVqHVY","executionInfo":{"status":"ok","timestamp":1745505257685,"user_tz":180,"elapsed":49,"user":{"displayName":"Carlos Bustillo","userId":"14895763228834044971"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["model_ft = torch.nn.Linear(10, 2)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# SGD con momentum\n","optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n","\n","# Scheduler: baja el LR cada 7 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n","\n","for epoch in range(15):\n","    optimizer_ft.step()\n","    exp_lr_scheduler.step()\n","    print(f\"Epoch {epoch+1}: LR = {optimizer_ft.param_groups[0]['lr']}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tpjL_MygqFoS","executionInfo":{"status":"ok","timestamp":1745505274282,"user_tz":180,"elapsed":7536,"user":{"displayName":"Carlos Bustillo","userId":"14895763228834044971"}},"outputId":"222cf9f9-7c11-4a2a-a3bf-94e2ff00d73b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: LR = 0.001\n","Epoch 2: LR = 0.001\n","Epoch 3: LR = 0.001\n","Epoch 4: LR = 0.001\n","Epoch 5: LR = 0.001\n","Epoch 6: LR = 0.001\n","Epoch 7: LR = 0.0001\n","Epoch 8: LR = 0.0001\n","Epoch 9: LR = 0.0001\n","Epoch 10: LR = 0.0001\n","Epoch 11: LR = 0.0001\n","Epoch 12: LR = 0.0001\n","Epoch 13: LR = 0.0001\n","Epoch 14: LR = 1e-05\n","Epoch 15: LR = 1e-05\n"]}]},{"cell_type":"markdown","source":["Lo que vemos es lo siguiente:\n","```\n","Epoch | LR\n","1‚Äì7   | 0.001\n","8‚Äì14  | 0.0001\n","15+   | 0.00001\n","```"],"metadata":{"id":"r_Beg57CrXg3"}},{"cell_type":"markdown","source":["## 4) üí° Tips"],"metadata":{"id":"z65aoeKRXm-p"}},{"cell_type":"markdown","source":["- Eleg√≠ la funci√≥n de p√©rdida adecuada: clasificaci√≥n, regresi√≥n, etc.\n","- Monitore√° `train loss` vs `val loss`. Si `train ‚Üì` pero `val ‚Üë`, est√°s sobreajustando.\n","- Experiment√° con diferentes optimizadores y programadores de LR."],"metadata":{"id":"RDLw-VTmXmHw"}}]}